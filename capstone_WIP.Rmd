---
title: "Capstone_WIP"
author: "Vijay Goel"
date: "March 12, 2015"
output: html_document
---

#### Synopsis
This project is about Text mining data from blogs, news and twitter, to predict next word as a user types. This document covers first part, which shares results from initial exploration of data, and plan for future prediction and presentation. This document has specified tasks, assumptions and approach, summary statistics from data cleaning, interesting results from early explorations, and plan for future.
Please let me know of any feedback! Happy to incorporate.

#####Tasks to accomplish
Tokenization and Filtering - Data manually downloaded from links provided in course assignment and unzipped. US language selelcted, and functions created for tokenization and filtering. Logic shared below, followed by results. Scripts are loaded at https://github.com/goelvijayk/capstone/blob/master/scratch_capstone.R

#####Logic used
Capital/Lower case: All converted to lower case, and value of capitalization was ignored, towards prediction of next word. It is assumed that abbreviations, formatting (e.g. of first word in sentence) are of very low importance.

Punctuation: handled by standard tm_map filters. All punctuation was removed. This spoils cases like I'd, wasn't, phrases within quotation marks etc. Impact assumed to be small.

Times, dates, numbers and currency: Assumed not useful for prediction. All were removed.

Typos: not addressed. Can try stemming, and recombining the words later. Or try connecting to an existing dictionary. Skipped for now.

Garbage/wrong language: Assumed to have non-alphabets, non-numbers. Any element with any character outside of English was treated as foreign, and entire entry was removed from raw data. This over-removes entries, but volume being small, this likely is ok.

Profanity: Profane words were assumed to be something we don't want to suggest to users. Hence, they were removed altogether. Couple of standard profane word lists were downloaded from internet, and used as filters. Links are available in R script for reference.

Special characters: e.g. -, ., !, @ etc. were manually listed and removed. These are fairly hard to predict, and they mix the words up. Ignored for now. This also creates problems e.g. ex-wife. 


#### Results
```{r echo=FALSE, warning=FALSE, message=FALSE}
source("~/Documents/Training/Capstone_Swiftkey/scripts/capstone/scratch_capstone.R")
initialize(10000)
clean<- cleanTokenizeCorpus(clean)
print(messages)
plot(table(allWords), xlab="Frequency of words", ylab="# of words in sample", main="Freq pattern of words")
print(paste("Top words are - "))
makeCloud(50)
```

###Pending
* Report interesting findings and insert plots
* What is the best set of features you might use to predict the next word?
* plan for creating prediction algorithm and shiny app

Tasks to accomplish:
Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.
Questions to consider

Some words are more frequent than others - what are the distributions of word frequencies? 
What are the frequencies of 2-grams and 3-grams in the dataset? 
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 
How do you evaluate how many of the words come from foreign languages? 
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

clean html url
